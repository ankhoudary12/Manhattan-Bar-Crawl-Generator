{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Purpose of This Notebook will be To highlight the Steps Necessary to Make the Manhattan Bar Crawl Generator APP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Data was scraped from Yelp using Scrapy, the code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import time\n",
    "\n",
    "neighborhoods=['https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Manhattan',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Alphabet_City',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Battery_Park',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Central_Park',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Chelsea',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Chinatown',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Civic_Center',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:East_Harlem',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:East_Village',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Financial_District',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Flatiron',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Gramercy',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Greenwich_Village',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Harlem',\n",
    " \"https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Hell's_Kitchen\",\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Inwood',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Kips_Bay',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Koreatown',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Little_Italy',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Lower_East_Side',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Manhattan_Valley',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Marble_Hill',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Meatpacking_District',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Midtown_East',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Midtown_West',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Morningside_Heights',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Murray_Hill',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:NoHo',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Nolita',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Roosevelt_Island',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:SoHo',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:South_Street_Seaport',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:South_Village',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Stuyvesant_Town',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Theater_District',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:TriBeCa',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Two_Bridges',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Union_Square',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Upper_East_Side',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Upper_West_Side',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Washington_Heights',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:West_Village',\n",
    " 'https://www.yelp.com/search?start=0&cflt=nightlife&l=p:NY:New_York:Manhattan:Yorkville']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class YelpSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'yelp_bar_scraper'\n",
    "\n",
    "    custom_settings = {\n",
    "        \"DOWNLOAD_DELAY\": 3,\n",
    "        \"CONCURRENT_REQUESTS_PER_DOMAIN\": 3,\n",
    "        \"HTTPCACHE_ENABLED\": True,\n",
    "        'ROBOTSTXT_OBEY': False\n",
    "\n",
    "    }\n",
    "\n",
    "    start_urls = ['https://www.yelp.com']\n",
    "\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        for link in neighborhoods:\n",
    "\n",
    "            try:\n",
    "                yield scrapy.Request(\n",
    "                url=link,\n",
    "                callback=self.parse_neighborhood\n",
    "                )\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "    def parse_neighborhood(self, response):\n",
    "\n",
    "        for href in response.xpath('//span[@class=\"indexed-biz-name\"]/a/@href').extract():\n",
    "\n",
    "            try:\n",
    "                yield scrapy.Request(\n",
    "                url='https://www.yelp.com' + href,\n",
    "                callback=self.parse_bar,\n",
    "                meta={'url': 'https://www.yelp.com' + href}\n",
    "            )\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "\n",
    "        next_url = 'https://www.yelp.com/'+ response.xpath('//div[@class=\"arrange_unit\"]/a/@href').extract()[-1]\n",
    "\n",
    "        yield scrapy.Request(\n",
    "            url=next_url,\n",
    "            callback=self.parse_neighborhood\n",
    "        )\n",
    "\n",
    "    def parse_bar(self, response):\n",
    "\n",
    "        url = response.request.meta['url']\n",
    "\n",
    "        name = response.xpath('//h1/text()').extract()[0]\n",
    "\n",
    "        neighborhood = response.xpath('//span[@class=\"neighborhood-str-list\"]/text()').extract()[0]\n",
    "\n",
    "        address= response.xpath('//strong[@class=\"street-address\"]/address/text()').extract()\n",
    "\n",
    "        price =  response.xpath('//span[@class=\"business-attribute price-range\"]/text()').extract()[0]\n",
    "\n",
    "        reviews = response.xpath('//div[@class=\"review-content\"]/p/text()').extract()\n",
    "\n",
    "        feature_names= response.xpath('//div[@class=\"short-def-list\"]/dl/dt/text()').extract()\n",
    "\n",
    "        feature_atts= response.xpath('//div[@class=\"short-def-list\"]/dl/dd/text()').extract()\n",
    "\n",
    "\n",
    "\n",
    "        yield {\n",
    "            'url': url,\n",
    "            'name': name,\n",
    "            'neighborhood': neighborhood,\n",
    "            'address': address,\n",
    "            'price': price,\n",
    "            'reviews': reviews,\n",
    "            'feature_names': feature_names,\n",
    "            'feature_atts': feature_atts\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_json('yelp_bar.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_up(x):\n",
    "    '''This function will take a list as input. It will strip all whitespace and \\n from the first element of the string.'''\n",
    "    try:\n",
    "        import re\n",
    "        x=re.sub('\\s+',' ',x).strip()\n",
    "        return x\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_up_neighborhood(x):\n",
    "    '''Takes in string, removes whitespace, \\n and only selects first neighborhood listed'''\n",
    "    try:\n",
    "        import re\n",
    "        x=re.sub('\\s+',' ',x).strip()\n",
    "        if ',' in x:\n",
    "            return x[:x.index(',')]\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['address']=df.apply(lambda x: ' '.join(clean_up2(x['address'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['name']=df.apply(lambda x: ' '.join(list(x['name'].split())),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['neighborhood']=df.apply(lambda x: clean_up_neighborhood(x['neighborhood']),axis=1 ) #some bars list two neighborhoods, I chose first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_up2(x):\n",
    "    '''This function will take a list of elements as input. It will strip all whitespace and \\n from all elements of the string.'''\n",
    "    a=[]\n",
    "    for i in x:\n",
    "        try:\n",
    "            import re\n",
    "            x=re.sub('\\s+',' ',i).strip()\n",
    "            a.append(x)\n",
    "        except:\n",
    "            return np.nan\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reviews_df=df[['name','reviews']]\n",
    "reviews_df.to_pickle('yelp_reviews.pickle') save this to work in separate notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Now use the GMaps geocoding api to get latitude and longitude for each venue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import googlemaps\n",
    "gmaps = googlemaps.Client(key='mykey')\n",
    "\n",
    "def gmaps_geocode(address):\n",
    "    '''This function takes an address and returns tuple pair of lat,lng coordinates using gmaps api'''\n",
    "    \n",
    "    import googlemaps\n",
    "    gmaps=googlemaps.Client(key='mykey')\n",
    "    \n",
    "    coord=[]\n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(address)\n",
    "    \n",
    "        coord.append(geocode_result[0]['geometry']['location']['lat'])\n",
    "        coord.append(geocode_result[0]['geometry']['location']['lng'])\n",
    "        \n",
    "        return coord\n",
    "   \n",
    "    except:\n",
    "        return None\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['coordinates']=df.apply(lambda x: gmaps_geocode(x['address']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('bars_complete.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Topic Modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle('yelp_reviews.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_up3(x):\n",
    "    '''This function will take a list of elements as input. It will strip \\xa0 from all elements of the string.'''\n",
    "    import unicodedata\n",
    "    a=[]\n",
    "    for i in x:\n",
    "        i=unicodedata.normalize('NFKD', i)\n",
    "        a.append(i)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def joiner(x):\n",
    "    '''This function will take a list of strings as input. It will output a list with a single long string.'''\n",
    "    s=''\n",
    "    for i in x:\n",
    "        s+=i\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['reviews']=df.apply(lambda x: joiner(x['reviews']),axis=1) ###need to have each bar's reviews as big string in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop('review',axis=1,inplace=True) ###accidentally created unwanted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('reviews_for_topics.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(  \n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "    min_df=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer.fit(df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts=count_vectorizer.transform(df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10, \n",
    "    learning_method=\"online\", \n",
    "    verbose=1, \n",
    "    max_iter=5, \n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_top_words(lda, count_vectorizer.get_feature_names(), 10) ###viewing our 10 topics and associated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(  \n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "    min_df=10,\n",
    "    max_df=0.8)    ###lets try running model again with higher min_df, and setting a max_df (get rid of words like food,good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer.fit(df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts=count_vectorizer.transform(df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts.shape ##we removed over half of our words, let's see how this turns out!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10, \n",
    "    learning_method=\"online\", \n",
    "    verbose=1, \n",
    "    max_iter=5, \n",
    "    n_jobs=-1\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_top_words(lda, count_vectorizer.get_feature_names(), 10) ###viewing our 10 topics and associated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#lets try setting n_topics to 20 to get better granularity/more discrete topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda2 = LatentDirichletAllocation(\n",
    "    n_components=20, \n",
    "    learning_method=\"online\", \n",
    "    verbose=1, \n",
    "    max_iter=5, \n",
    "    n_jobs=-1\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda2.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_top_words(lda2, count_vectorizer.get_feature_names(), 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer3 = CountVectorizer(  \n",
    "    ngram_range=(1, 2),\n",
    "    stop_words='english',\n",
    "    token_pattern=\"\\\\b[a-z][a-z]+\\\\b\",\n",
    "    min_df=10,\n",
    "    max_df=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer3.fit(df['reviews'])\n",
    "counts3=count_vectorizer3.transform(df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda3 = LatentDirichletAllocation(\n",
    "    n_components=10, \n",
    "    learning_method=\"online\", \n",
    "    verbose=1, \n",
    "    max_iter=5, \n",
    "    n_jobs=-1\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_top_words(lda3, count_vectorizer3.get_feature_names(), 15) ##let's print 15 words to see if that makes topics more obvious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Try one last time with 30 topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda5= LatentDirichletAllocation(\n",
    "    n_components=30, \n",
    "    learning_method=\"online\", \n",
    "    verbose=1, \n",
    "    max_iter=5, \n",
    "    n_jobs=-1\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lda5.fit(counts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_top_words(lda5, count_vectorizer3.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Topic Modeling With NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle('reviews_for_topics.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(max_df=0.3, min_df=0.01, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.fit(df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts=tfidf_vectorizer.transform(df['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nmf= NMF (n_components=30,\n",
    "         random_state=1, \n",
    "          alpha=.1, l1_ratio=.5,\n",
    "          init='nndsvd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nmf.fit(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print_top_words(nmf, tfidf_vectorizer.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topic_probs=nmf.transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topic_df=pd.DataFrame(topic_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topics=['speakeasy','club/dance','irish','mexican','rooftop','hookah',\\\n",
    "       'italian','karaoke','concert','pizza','jazz/blues','comedy',\\\n",
    "       'theater','dive','hotel','brunch','pool/games','seafood','sushi',\\\n",
    "       'korean','cafe','sports_1','french','ramen','steak','thai',\\\n",
    "       'smoking','lgbt','african/caribbean','sports_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "topic_df.columns=topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=pd.merge(df,topic_df, left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df.to_pickle('bar_topics.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#df.to_pickle('bar_topics.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics=df.drop(['name','reviews'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists=cosine_similarity(df_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_df=pd.DataFrame(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_df.index=df_topics.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_df.columns=df_topics.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_df.to_pickle('bar_similarity.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_df=pd.read_pickle('bars_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bt=bars_df.join(df_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=15)\n",
    "heat=sns.heatmap(df_bt.groupby('neighborhood').mean(), cmap=\"BuPu\")\n",
    "plt.tight_layout()\n",
    "#plt.savefig('heatmap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_df=pd.read_pickle('bar_similarity.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('bars_complete.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_df=df[['name','neighborhood','price','url','coordinates']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_df.set_index(bars_df.name, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_bars(bar, d=1 ,number=10):\n",
    "    from math import radians, cos, sin, asin, sqrt\n",
    "    import pandas as pd\n",
    "\n",
    "    def haversine(lon1, lat1, lon2, lat2):\n",
    "        \"\"\"\n",
    "        Calculate the great circle distance between two points \n",
    "        on the earth (specified in decimal degrees)\n",
    "        \"\"\"\n",
    "        # convert decimal degrees to radians \n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "        # haversine formula \n",
    "        dlon = lon2 - lon1 \n",
    "        dlat = lat2 - lat1 \n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        r = 3956 # For miles\n",
    "        return c * r\n",
    "\n",
    "    def is_in_area(center_lon, center_lat, test_lon, test_lat, radius = 1):\n",
    "        a = haversine(center_lon, center_lat, test_lon, test_lat)\n",
    "        if a <= radius:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    import jellyfish\n",
    "\n",
    "    def get_closest_match(x, list_strings):\n",
    "\n",
    "        best_score= 0\n",
    "        best_match=None\n",
    "\n",
    "        for title in list_strings:\n",
    "\n",
    "            current_score=jellyfish.jaro_winkler(x, title)\n",
    "\n",
    "            if current_score > best_score:\n",
    "                best_score = current_score\n",
    "                best_match = title \n",
    "\n",
    "        if best_score > 0.75:\n",
    "            return best_match\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    bar = get_closest_match(bar, bars_df.index)\n",
    "    \n",
    "    bool_array=[]\n",
    "    \n",
    "    for i in bars_df['coordinates']:\n",
    "        \n",
    "        try:\n",
    "            bool_array.append(is_in_area(bars_df[bars_df.index==bar]['coordinates'][0][0]\\\n",
    "                                         ,bars_df[bars_df.index==bar]['coordinates'][0][1],\\\n",
    "                                         i[0],i[1], radius=d))\n",
    "        \n",
    "        except:\n",
    "            bool_array.append(None)\n",
    "            \n",
    "    bool_dict=dict(zip(bars_df.index,bool_array))\n",
    "    \n",
    "    bool_df=pd.Series(bool_dict).to_frame()\n",
    "    \n",
    "    matching_bars=sims_df.join(bool_df[bool_df[0]==True], how='inner')\n",
    "    \n",
    "    final_bars=bars_df.join(matching_bars[bar].sort_values(ascending=False)[0:number+1], how='right')\n",
    "    \n",
    "    return final_bars.sort_values(by=bar, ascending=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_bars(df):\n",
    "    bars=[]\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        a=row.index\n",
    "        a={}\n",
    "        a['name']=row['name']\n",
    "        a['neighborhood']=row['neighborhood']\n",
    "        a['price']=row['price']\n",
    "        a['lat']=row['coordinates'][0]\n",
    "        a['lng']=row['coordinates'][1]\n",
    "        a['url']=row['url']\n",
    "        bars.append(a)\n",
    "    \n",
    "    return bars\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is the final function used in my bar app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_bars(bar, d=1, number=10):\n",
    "    \n",
    "    '''Will ultimately take a bar, radius, and number (n) and return closest (n) bars in radius (d)  in terms of \n",
    "    vibe/type  to iniitial bar'''\n",
    "    \n",
    "    def find_nearest_bars(bar, d=1 ,number=10):\n",
    "        from math import radians, cos, sin, asin, sqrt\n",
    "        import pandas as pd\n",
    "\n",
    "        def haversine(lon1, lat1, lon2, lat2):\n",
    "            \"\"\"\n",
    "            Calculate the great circle distance between two points \n",
    "            on the earth (specified in decimal degrees)\n",
    "            \"\"\"\n",
    "            # convert decimal degrees to radians \n",
    "            lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "            # haversine formula \n",
    "            dlon = lon2 - lon1 \n",
    "            dlat = lat2 - lat1 \n",
    "            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "            c = 2 * asin(sqrt(a)) \n",
    "            r = 3956 # For miles\n",
    "            return c * r\n",
    "\n",
    "        def is_in_area(center_lon, center_lat, test_lon, test_lat, radius = 1):\n",
    "            '''Will determine if lat,lng is within radius of desired point'''\n",
    "            a = haversine(center_lon, center_lat, test_lon, test_lat)\n",
    "            if a <= radius:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "\n",
    "        import jellyfish\n",
    "\n",
    "        def get_closest_match(x, list_strings):\n",
    "            \n",
    "            '''Will return string that best matches list of strings according to jaro winkler score'''\n",
    "\n",
    "            best_score= 0\n",
    "            best_match=None\n",
    "\n",
    "            for title in list_strings:\n",
    "\n",
    "                current_score=jellyfish.jaro_winkler(x, title)\n",
    "\n",
    "                if current_score > best_score:\n",
    "                    best_score = current_score\n",
    "                    best_match = title \n",
    "\n",
    "            if best_score > 0.75:\n",
    "                return best_match\n",
    "\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "\n",
    "        bar = get_closest_match(bar, bars_df.index)\n",
    "\n",
    "        bool_array=[] \n",
    "\n",
    "        for i in bars_df['coordinates']: # make a boolean array of whether bars are within radius of desired bar\n",
    "\n",
    "            try:\n",
    "                bool_array.append(is_in_area(bars_df[bars_df.index==bar]['coordinates'][0][0]\\\n",
    "                                             ,bars_df[bars_df.index==bar]['coordinates'][0][1],\\\n",
    "                                             i[0],i[1], radius=d))\n",
    "\n",
    "            except:\n",
    "                bool_array.append(None)\n",
    "\n",
    "        bool_dict=dict(zip(bars_df.index,bool_array)) # create dict of bar name:True/False whether in desired radius\n",
    "\n",
    "        bool_df=pd.Series(bool_dict).to_frame()\n",
    "\n",
    "        matching_bars=sims_df.join(bool_df[bool_df[0]==True], how='inner') #dataframe containing only bars within radius\n",
    "\n",
    "        final_bars=bars_df.join(matching_bars[bar].sort_values(ascending=False)[0:number+1], how='right')\n",
    "\n",
    "        return final_bars.sort_values(by=bar, ascending=False) #returns list of bars by closest cosine similarity\n",
    "\n",
    "\n",
    "    def pack_bars(df):\n",
    "        '''Will take resuls of finding nearest bars and return a list of bar dictionaries containing relevant info'''\n",
    "        \n",
    "        import operator\n",
    "        \n",
    "        bars=[]\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            a=row.index\n",
    "            a={}\n",
    "            a['name']=row['name']\n",
    "            a['neighborhood']=row['neighborhood']\n",
    "            a['price']=row['price']\n",
    "            a['lat']=row['coordinates'][0]\n",
    "            a['lng']=row['coordinates'][1]\n",
    "            a['url']=row['url']\n",
    "            bars.append(a)\n",
    "\n",
    "        bars.sort(key=operator.itemgetter('lng')) #arrange bars by longitude coordinate\n",
    "        \n",
    "        return bars\n",
    "\n",
    "\n",
    "    \n",
    "    return pack_bars(find_nearest_bars(bar, d, number))\n",
    "\n",
    "    \n",
    "    \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
